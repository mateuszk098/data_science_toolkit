<p style="
    font-family: 'JetBrains Mono';
    font-weight: bold;
    font-size: 200%;
    color: #F2F2F0;
    letter-spacing: 2px;
    text-align: left;
    border-radius: 8px;
">
    <b>Set of Notebooks from Kaggle Competitions</b>
</p>

<p style="
    font-size: 20px;
    font-family: 'JetBrains Mono';
">
    <b>What is this?</b> üìú
</p>

<p style="
    font-size: 16px;
    font-family: 'JetBrains Mono';
    margin-left: 20px;
    margin-right: 20px;
    margin-bottom: 20px;
">
    This repository contains different notebooks from <a href="https://www.kaggle.com/" style="color: #FFB74D;"><b>Kaggle</b></a> competitions in which I have participated and am currently taking part in. You can find here different types of notebooks, starting from very simple datasets like Titanic one up to challenges related to NLP like machine translation with Transformer architectures or competitions with money prizes.
</p>

<p style="
    font-size: 20px;
    font-family: 'JetBrains Mono';
">
    <b>Contents</b> üïµ
</p>

<ul style="
    font-size: 16px;
    font-family: 'JetBrains Mono';
    margin-left: 0px;
    margin-right: 40px;
    margin-bottom: 20px;
">
    <li><a href="https://github.com/mateuszk098/kaggle_notebooks/tree/master/digit_recognizer" style="color: #FFB74D;"><b>Digit Recognizer</b></a> - <i>MNIST ("Modified National Institute of Standards and Technology") is the de facto ‚Äúhello world‚Äù dataset of computer vision. Since its release in 1999, this classic dataset of handwritten images has served as the basis for benchmarking classification algorithms. As new machine learning techniques emerge, MNIST remains a reliable resource for researchers and learners alike.</br>
    In this competition, your goal is to correctly identify digits from a dataset of tens of thousands of handwritten images. We‚Äôve curated a set of tutorial-style kernels which cover everything from regression to neural networks. We encourage you to experiment with different algorithms to learn first-hand what works well and how techniques compare.</i></br>
    <b>You can see more here: <a href="https://www.kaggle.com/competitions/digit-recognizer" style="color: #FFB74D;">https://www.kaggle.com/competitions/digit-recognizer</a>.</b></li>
    <li><a href="https://github.com/mateuszk098/kaggle_notebooks/tree/master/house_prices" style="color: #FFB74D;"><b>House Prices</b></a> - <i>Ask a home buyer to describe their dream house, and they probably won't begin with the height of the basement ceiling or the proximity to an east-west railroad. But this playground competition's dataset proves that much more influences price negotiations than the number of bedrooms or a white-picket fence.</br>
    With 79 explanatory variables describing (almost) every aspect of residential homes in Ames, Iowa, this competition challenges you to predict the final price of each home.</i></br>
    <b>You can see more here: <a href="https://www.kaggle.com/competitions/house-prices-advanced-regression-techniques" style="color: #FFB74D;">https://www.kaggle.com/competitions/house-prices-advanced-regression-techniques</a>.</b></li>
    <li><a href="https://github.com/mateuszk098/kaggle_notebooks/tree/master/icr" style="color: #FFB74D;"><b>ICR</b></a> - <i>In this competition, you‚Äôll work with measurements of health characteristic data to solve critical problems in bioinformatics. Based on minimal training, you‚Äôll create a model to predict if a person has any of three medical conditions, with an aim to improve on existing methods.</i></br>
    <b>You can see more here: <a href="https://www.kaggle.com/competitions/icr-identify-age-related-conditions" style="color: #FFB74D;">https://www.kaggle.com/competitions/icr-identify-age-related-conditions</a>.</b></li>
    <li><a href="https://github.com/mateuszk098/kaggle_notebooks/tree/master/mt_with_transformers" style="color: #FFB74D;"><b>MT with Transformers</b></a> - <i>This notebook aims to handle one of the natural language processing (NLP) challenges, i.e. machine translation. We will focus on employing the encoder-decoder RNN architecture and a disruptive approach to NLP, i.e. transformers architecture. To do that, we will use two English-French datasets. In the first part, we will focus on an easy dataset (around 180000 sentences, 12 MB), whereas in the second part, we will use the second dataset (about 22.5 million sentences, 8 GB). In this notebook, we translate English sentences into French ones. Therefore, we tackle the sequence-to-sequence (seq2seq) learning problem.</i></br>
    <b>You can see more here: <a href="https://www.kaggle.com/datasets/devicharith/language-translation-englishfrench" style="color: #FFB74D;">https://www.kaggle.com/datasets/devicharith/language-translation-englishfrench</a> and here <a href="https://www.kaggle.com/datasets/dhruvildave/en-fr-translation-dataset" style="color: #FFB74D;">https://www.kaggle.com/datasets/dhruvildave/en-fr-translation-dataset</a>.</b></li>
    <li><a href="https://github.com/mateuszk098/kaggle_notebooks/tree/master/optuna_examples" style="color: #FFB74D;"><b>Optuna Examples</b></a> - <i>Examples of how to set up and use Optuna hyperparameter optimization.</i></li>
    <li><a href="https://github.com/mateuszk098/kaggle_notebooks/tree/master/playground_series_s3e09" style="color: #FFB74D;"><b>Playground Series S3E09</b></a> - <i>The dataset for this competition (both train and test) was generated from a deep learning model trained on the Concrete Strength Prediction dataset. Feature distributions are close to, but not exactly the same, as the original. Feel free to use the original dataset as part of this competition, both to explore differences as well as to see whether incorporating the original in training improves model performance. You need to predict the strength of concrete based on its characteristics.</i></br>
    <b>You can see more here: <a href="https://www.kaggle.com/competitions/playground-series-s3e9" style="color: #FFB74D;">https://www.kaggle.com/competitions/playground-series-s3e9</a>.</b></li>
    <li><a href="https://github.com/mateuszk098/kaggle_notebooks/tree/master/playground_series_s3e10" style="color: #FFB74D;"><b>Playground Series S3E10</b></a> - <i>The dataset for this competition (both train and test) was generated from a deep learning model trained on the Pulsar Classification. Feature distributions are close to, but not exactly the same, as the original. Feel free to use the original dataset as part of this competition, both to explore differences as well as to see whether incorporating the original in training improves model performance. You need to predict whether the star is a pulsar (1) or not (0).</i></br>
    <b>You can see more here: <a href="https://www.kaggle.com/competitions/playground-series-s3e10" style="color: #FFB74D;">https://www.kaggle.com/competitions/playground-series-s3e10</a>.</b></li>
    <li><a href="https://github.com/mateuszk098/kaggle_notebooks/tree/master/playground_series_s3e12" style="color: #FFB74D;"><b>Playground Series S3E12</b></a> - <i>The dataset for this competition (both train and test) was generated from a deep learning model trained on the Kidney Stone Prediction based on Urine Analysis dataset. Feature distributions are close to, but not exactly the same, as the original. Feel free to use the original dataset as part of this competition, both to explore differences as well as to see whether incorporating the original in training improves model performance. You need to predict occurrence of kidney stones. Submissions are evaluated on area under the ROC curve between the predicted probability and the observed target.</i></br>
    <b>You can see more here: <a href="https://www.kaggle.com/competitions/playground-series-s3e12" style="color: #FFB74D;">https://www.kaggle.com/competitions/playground-series-s3e12</a>.</b></li>
    <li><a href="https://github.com/mateuszk098/kaggle_notebooks/tree/master/playground_series_s3e15" style="color: #FFB74D;"><b>Playground Series S3E15</b></a> - <i>The dataset for this competition (both train and test) was generated from a deep learning model trained on the Predicting Critical Heat Flux dataset. Feature distributions are close to, but not exactly the same, as the original. Feel free to use the original dataset as part of this competition, both to explore differences as well as to see whether incorporating the original in training improves model performance. You need to impute the missing values of the feature x_e_out [-] (equilibrium quality).</i></br>
    <b>You can see more here: <a href="https://www.kaggle.com/competitions/playground-series-s3e15" style="color: #FFB74D;">https://www.kaggle.com/competitions/playground-series-s3e15</a>.</b></li>
    <li><a href="https://github.com/mateuszk098/kaggle_notebooks/tree/master/spaceship_titanic" style="color: #FFB74D;"><b>Spaceship Titanic</b></a> - <i>Welcome to the year 2912, where your data science skills are needed to solve a cosmic mystery. We've received a transmission from four lightyears away and things aren't looking good. The Spaceship Titanic was an interstellar passenger liner launched a month ago. With almost 13,000 passengers on board, the vessel set out on its maiden voyage transporting emigrants from our solar system to three newly habitable exoplanets orbiting nearby stars. While rounding Alpha Centauri en route to its first destination - the torrid 55 Cancri E - the unwary Spaceship Titanic collided with a spacetime anomaly hidden within a dust cloud. Sadly, it met a similar fate as its namesake from 1000 years before. Though the ship stayed intact, almost half of the passengers were transported to an alternate dimension!</br>
    In this competition your task is to predict whether a passenger was transported to an alternate dimension during the Spaceship Titanic's collision with the spacetime anomaly. To help you make these predictions, you're given a set of personal records recovered from the ship's damaged computer system.</i></br>
    <b>You can see more here: <a href="https://www.kaggle.com/competitions/spaceship-titanic/overview" style="color: #FFB74D;">https://www.kaggle.com/competitions/spaceship-titanic/overview</a>.</b></li>
    <li><a href="https://github.com/mateuszk098/kaggle_notebooks/tree/master/titanic" style="color: #FFB74D;"><b>Titanic</b></a> - <i>The sinking of the Titanic is one of the most infamous shipwrecks in history. On April 15, 1912, during her maiden voyage, the widely considered "unsinkable" RMS Titanic sank after colliding with an iceberg. Unfortunately, there weren‚Äôt enough lifeboats for everyone onboard, resulting in the death of 1502 out of 2224 passengers and crew. While there was some element of luck involved in surviving, it seems some groups of people were more likely to survive than others.</br>
    In this challenge, we ask you to build a predictive model that answers the question: "what sorts of people were more likely to survive?" using passenger data (ie name, age, gender, socio-economic class, etc).</i></br>
    <b>You can see more here: <a href="https://www.kaggle.com/competitions/titanic/overview" style="color: #FFB74D;">https://www.kaggle.com/competitions/titanic/overview</a>.</b></li>
    <li><a href="https://github.com/mateuszk098/kaggle_notebooks/tree/master/weather_forecast" style="color: #FFB74D;"><b>Weather Forecast</b></a> - <i>In this notebook, our main focus is on the temperature variations that have occurred in Warsaw, Poland over the past 30 years. We will be examining a time series dataset and utilizing visualizations to better understand the data. Additionally, we will be employing ARIMA and Recurrent Neural Networks to predict the weather patterns for the chosen year.</i></br>
    <b>You can see more here: <a href="https://www.kaggle.com/datasets/mateuszk013/warsaw-daily-weather" style="color: #FFB74D;">https://www.kaggle.com/datasets/mateuszk013/warsaw-daily-weather</a>.</b></li>
</ul>
