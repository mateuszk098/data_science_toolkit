{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from multiprocessing import cpu_count\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import subprocess\n",
    "import shutil\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras import layers\n",
    "\n",
    "ON_KAGGLE = os.getenv(\"KAGGLE_KERNEL_RUN_TYPE\") is not None\n",
    "\n",
    "\n",
    "def download_dataset_from_kaggle(user, dataset, directory):\n",
    "    command = \"kaggle datasets download -d \"\n",
    "    filepath = directory / (dataset + \".zip\")\n",
    "\n",
    "    if not filepath.is_file():\n",
    "        subprocess.run((command + user + \"/\" + dataset).split())\n",
    "        filepath.parent.mkdir(parents=True, exist_ok=True)\n",
    "        shutil.unpack_archive(dataset + \".zip\", \"data\")\n",
    "        shutil.move(dataset + \".zip\", \"data\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 0 ns\n",
      "Wall time: 999 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "easy_dataset_user = \"devicharith\"\n",
    "easy_dataset = \"language-translation-englishfrench\"\n",
    "\n",
    "hard_dataset_user = \"dhruvildave\"\n",
    "hard_dataset = \"en-fr-translation-dataset\"\n",
    "\n",
    "data_dir = Path(\"data\")\n",
    "\n",
    "if not ON_KAGGLE:\n",
    "    download_dataset_from_kaggle(easy_dataset_user, easy_dataset, data_dir)\n",
    "    download_dataset_from_kaggle(hard_dataset_user, hard_dataset, data_dir)\n",
    "    easy_dataset_path = data_dir / \"eng_-french.csv\"\n",
    "    hard_dataset_path = data_dir / \"en-fr.csv\"\n",
    "else:\n",
    "    easy_dataset_path = Path(\"/kaggle/input/language-translation-englishfrench/eng_-french.csv\")\n",
    "    hard_dataset_path = Path(\"/kaggle/input/en-fr-translation-dataset/en-fr.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>English words/sentences</th>\n",
       "      <th>French words/sentences</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2785</th>\n",
       "      <td>Take a seat.</td>\n",
       "      <td>Prends place !</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29880</th>\n",
       "      <td>I wish Tom was here.</td>\n",
       "      <td>J'aimerais que Tom soit là.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53776</th>\n",
       "      <td>How did the audition go?</td>\n",
       "      <td>Comment s'est passée l'audition ?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>154386</th>\n",
       "      <td>I've no friend to talk to about my problems.</td>\n",
       "      <td>Je n'ai pas d'ami avec lequel je puisse m'entr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149823</th>\n",
       "      <td>I really like this skirt. Can I try it on?</td>\n",
       "      <td>J'aime beaucoup cette jupe, puis-je l'essayer ?</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             English words/sentences  \\\n",
       "2785                                    Take a seat.   \n",
       "29880                           I wish Tom was here.   \n",
       "53776                       How did the audition go?   \n",
       "154386  I've no friend to talk to about my problems.   \n",
       "149823    I really like this skirt. Can I try it on?   \n",
       "\n",
       "                                   French words/sentences  \n",
       "2785                                       Prends place !  \n",
       "29880                         J'aimerais que Tom soit là.  \n",
       "53776                   Comment s'est passée l'audition ?  \n",
       "154386  Je n'ai pas d'ami avec lequel je puisse m'entr...  \n",
       "149823    J'aime beaucoup cette jupe, puis-je l'essayer ?  "
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "easy_dataset = pd.read_csv(easy_dataset_path, encoding=\"utf-8\", engine=\"pyarrow\")\n",
    "easy_dataset = easy_dataset.sample(len(easy_dataset), random_state=42)\n",
    "easy_dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(158059,)\n",
      "(17562,)\n"
     ]
    }
   ],
   "source": [
    "sentences_en = easy_dataset[\"English words/sentences\"].to_numpy()\n",
    "sentences_fr = easy_dataset[\"French words/sentences\"].to_numpy()\n",
    "\n",
    "validation_size = 0.1\n",
    "valid_len = int(validation_size * len(easy_dataset))\n",
    "\n",
    "sentences_en_train = sentences_en[:-valid_len]\n",
    "sentences_fr_train = sentences_fr[:-valid_len]\n",
    "\n",
    "sentences_en_valid = sentences_en[-valid_len:]\n",
    "sentences_fr_valid = sentences_fr[-valid_len:]\n",
    "\n",
    "print(sentences_en_train.shape)\n",
    "print(sentences_en_valid.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_input_and_target(sentences_en, sentences_fr):\n",
    "    return (sentences_en, b\"startofseq \" + sentences_fr), sentences_fr + b\" endofseq\"\n",
    "\n",
    "\n",
    "def from_sentences_dataset(\n",
    "    sentences_en,\n",
    "    sentences_fr,\n",
    "    batch_size=32,\n",
    "    shuffle=False,\n",
    "    shuffle_buffer_size=10_000,\n",
    "    seed=None,\n",
    "):\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((sentences_en, sentences_fr))\n",
    "    dataset = dataset.map(prepare_input_and_target)\n",
    "    if shuffle:\n",
    "        dataset = dataset.shuffle(shuffle_buffer_size, seed=seed)\n",
    "    return dataset.batch(batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "easy_train_ds = from_sentences_dataset(sentences_en_train, sentences_fr_train)\n",
    "easy_valid_ds = from_sentences_dataset(sentences_en_valid, sentences_fr_valid)\n",
    "\n",
    "# list(easy_train_ds.take(1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "4940/4940 [==============================] - 174s 34ms/step - loss: 0.5073 - accuracy: 0.3785 - val_loss: 0.4077 - val_accuracy: 0.4598\n",
      "Epoch 2/10\n",
      "4940/4940 [==============================] - 167s 34ms/step - loss: 0.3220 - accuracy: 0.5321 - val_loss: 0.3119 - val_accuracy: 0.5549\n",
      "Epoch 3/10\n",
      "4940/4940 [==============================] - 167s 34ms/step - loss: 0.2431 - accuracy: 0.6157 - val_loss: 0.2693 - val_accuracy: 0.6035\n",
      "Epoch 4/10\n",
      "4940/4940 [==============================] - 169s 34ms/step - loss: 0.1967 - accuracy: 0.6716 - val_loss: 0.2513 - val_accuracy: 0.6269\n",
      "Epoch 5/10\n",
      "4940/4940 [==============================] - 170s 34ms/step - loss: 0.1655 - accuracy: 0.7127 - val_loss: 0.2444 - val_accuracy: 0.6393\n",
      "Epoch 6/10\n",
      "4940/4940 [==============================] - 169s 34ms/step - loss: 0.1421 - accuracy: 0.7461 - val_loss: 0.2435 - val_accuracy: 0.6462\n",
      "Epoch 7/10\n",
      "4940/4940 [==============================] - 167s 34ms/step - loss: 0.1238 - accuracy: 0.7731 - val_loss: 0.2459 - val_accuracy: 0.6491\n",
      "Epoch 8/10\n",
      "4940/4940 [==============================] - 160s 32ms/step - loss: 0.1094 - accuracy: 0.7954 - val_loss: 0.2513 - val_accuracy: 0.6495\n",
      "Epoch 9/10\n",
      "4940/4940 [==============================] - 158s 32ms/step - loss: 0.0979 - accuracy: 0.8135 - val_loss: 0.2581 - val_accuracy: 0.6491\n",
      "Epoch 10/10\n",
      "4940/4940 [==============================] - 164s 33ms/step - loss: 0.0888 - accuracy: 0.8274 - val_loss: 0.2649 - val_accuracy: 0.6481\n"
     ]
    }
   ],
   "source": [
    "keras.backend.clear_session()\n",
    "tf.random.set_seed(42)  # Ensure reproducibility on CPU.\n",
    "\n",
    "easy_train_ds = from_sentences_dataset(sentences_en_train, sentences_fr_train)\n",
    "easy_valid_ds = from_sentences_dataset(sentences_en_valid, sentences_fr_valid)\n",
    "\n",
    "basic_encoder_decoder = BasicEncoderDecoder()\n",
    "history = adapt_compile_and_fit(basic_encoder_decoder, easy_train_ds, easy_valid_ds, epochs=10)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 0 ns\n",
      "Wall time: 0 ns\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "chunk_size = 100_000\n",
    "chunks_dir = Path(\"data_chunks\")\n",
    "\n",
    "if not os.path.exists(chunks_dir):\n",
    "    chunks_dir.mkdir(parents=True)\n",
    "    chunks = pd.read_csv(full_data_path, chunksize=chunk_size, encoding=\"utf-8\")\n",
    "    for i, chunk in enumerate(chunks):\n",
    "        chunk_path = chunks_dir / f\"en-fr-chunk-{i:03}.csv\"\n",
    "        chunk.to_csv(chunk_path, index=False, encoding=\"utf-8\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "en,fr\n",
      "Changing Lives | Changing Society | How It Works | Technology Drives Change Home | Concepts | Teachers | Search | Overview | Credits | HHCC Web | Reference | Feedback Virtual Museum of Canada Home Page,\"Il a transformé notre vie | Il a transformé la société | Son fonctionnement | La technologie, moteur du changement Accueil | Concepts | Enseignants | Recherche | Aperçu | Collaborateurs | Web HHCC | Ressources | Commentaires Musée virtuel du Canada\"\n",
      "Site map,Plan du site\n",
      "Feedback,Rétroaction\n",
      "Credits,Crédits\n"
     ]
    }
   ],
   "source": [
    "filepaths = [f\"{chunks_dir}/{chunk_file}\" for chunk_file in os.listdir(chunks_dir)]\n",
    "\n",
    "with open(filepaths[0], encoding=\"utf8\") as f:\n",
    "    for line in f.readlines()[:5]:\n",
    "        print(line, end=\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_csv_line(line):\n",
    "    defaults = 2 * [tf.constant(\"\", dtype=tf.string)]\n",
    "    fields = tf.io.decode_csv(line, record_defaults=defaults)\n",
    "    return tf.stack(fields[0]), tf.stack(fields[1])\n",
    "\n",
    "\n",
    "def prepare_input_and_target(sentences_en, sentences_fr):\n",
    "    return (sentences_en, b\"startofseq \" + sentences_fr), sentences_fr + b\" endofseq\"\n",
    "\n",
    "\n",
    "def from_csv_files_dataset(\n",
    "    filepaths,\n",
    "    n_readers=12,\n",
    "    n_read_threads=12,\n",
    "    n_parse_threads=12,\n",
    "    batch_size=32,\n",
    "    shuffle=False,\n",
    "    shuffle_buffer_size=10_000,\n",
    "    seed=42,\n",
    "):\n",
    "    dataset = tf.data.Dataset.list_files(filepaths, seed=seed)\n",
    "    dataset = dataset.interleave(  # type: ignore\n",
    "        lambda filepath: tf.data.TextLineDataset(filepath).skip(1),\n",
    "        cycle_length=n_readers,\n",
    "        num_parallel_calls=n_read_threads,\n",
    "    )\n",
    "    dataset = dataset.map(parse_csv_line, num_parallel_calls=n_parse_threads)\n",
    "    dataset = dataset.map(prepare_input_and_target, num_parallel_calls=n_parse_threads)\n",
    "    if shuffle:\n",
    "        dataset = dataset.shuffle(shuffle_buffer_size, seed=seed)\n",
    "    return dataset.batch(batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicEncoderDecoder(keras.Model):\n",
    "    def __init__(\n",
    "        self,\n",
    "        vocabulary_size=5000,\n",
    "        max_length=50,\n",
    "        embedding_size=128,\n",
    "        n_units_lstm=512,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "        self.vectorization_layer_en = layers.TextVectorization(\n",
    "            vocabulary_size, output_sequence_length=max_length\n",
    "        )\n",
    "        self.vectorization_layer_es = layers.TextVectorization(\n",
    "            vocabulary_size, output_sequence_length=max_length\n",
    "        )\n",
    "\n",
    "        self.encoder_embedding_layer = layers.Embedding(\n",
    "            vocabulary_size, embedding_size, mask_zero=True\n",
    "        )\n",
    "        self.decoder_embedding_layer = layers.Embedding(\n",
    "            vocabulary_size, embedding_size, mask_zero=True\n",
    "        )\n",
    "\n",
    "        self.encoder = layers.LSTM(n_units_lstm, return_state=True)\n",
    "        self.decoder = layers.LSTM(n_units_lstm, return_sequences=True)\n",
    "\n",
    "        self.output_layer = layers.Dense(vocabulary_size, activation=\"softmax\")\n",
    "\n",
    "    def call(self, inputs):\n",
    "        encoder_inputs, decoder_inputs = inputs\n",
    "\n",
    "        encoder_input_ids = self.vectorization_layer_en(encoder_inputs)\n",
    "        decoder_input_ids = self.vectorization_layer_es(decoder_inputs)\n",
    "\n",
    "        encoder_embeddings = self.encoder_embedding_layer(encoder_input_ids)\n",
    "        decoder_embeddings = self.decoder_embedding_layer(decoder_input_ids)\n",
    "\n",
    "        encoder_output, *encoder_state = self.encoder(encoder_embeddings)\n",
    "        decoder_output = self.decoder(decoder_embeddings, initial_state=encoder_state)\n",
    "\n",
    "        return self.output_layer(decoder_output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adapt_compile_and_fit(model, train_dataset, valid_dataset, epochs=5):\n",
    "    model.vectorization_layer_en.adapt(\n",
    "        train_dataset.map(lambda sentences, target: sentences[0])\n",
    "    )\n",
    "    model.vectorization_layer_es.adapt(\n",
    "        valid_dataset.map(lambda sentences, target: sentences[1] + b\" endofseq\")\n",
    "    )\n",
    "\n",
    "    train_dataset_prepared = train_dataset.map(\n",
    "        lambda sentences, target: (sentences, model.vectorization_layer_es(target))\n",
    "    ).prefetch(1)\n",
    "\n",
    "    valid_dataset_prepared = valid_dataset.map(\n",
    "        lambda sentences, target: (sentences, model.vectorization_layer_es(target))\n",
    "    ).prefetch(1)\n",
    "\n",
    "    model.compile(\n",
    "        loss=\"sparse_categorical_crossentropy\", optimizer=\"nadam\", metrics=[\"accuracy\"]\n",
    "    )\n",
    "    history = model.fit(\n",
    "        train_dataset_prepared, epochs=epochs, validation_data=valid_dataset_prepared\n",
    "    )\n",
    "\n",
    "    return history\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "3125/3125 [==============================] - 181s 56ms/step - loss: 1.8296 - accuracy: 0.2846 - val_loss: 2.4139 - val_accuracy: 0.1999\n",
      "Epoch 2/5\n",
      "3125/3125 [==============================] - 173s 55ms/step - loss: 1.5731 - accuracy: 0.3293 - val_loss: 2.2696 - val_accuracy: 0.2218\n",
      "Epoch 3/5\n",
      "3125/3125 [==============================] - 172s 55ms/step - loss: 1.4491 - accuracy: 0.3564 - val_loss: 2.1983 - val_accuracy: 0.2348\n",
      "Epoch 4/5\n",
      "3125/3125 [==============================] - 175s 56ms/step - loss: 1.3590 - accuracy: 0.3786 - val_loss: 2.1618 - val_accuracy: 0.2447\n",
      "Epoch 5/5\n",
      "3125/3125 [==============================] - 179s 57ms/step - loss: 1.2895 - accuracy: 0.3963 - val_loss: 2.1535 - val_accuracy: 0.2476\n"
     ]
    }
   ],
   "source": [
    "keras.backend.clear_session()\n",
    "tf.random.set_seed(42)  # Ensure reproducibility on CPU.\n",
    "\n",
    "train_ds = from_csv_files_dataset(filepaths[0:1], shuffle=True)\n",
    "valid_ds = from_csv_files_dataset(filepaths[1:2])\n",
    "\n",
    "basic_encoder_decoder = BasicEncoderDecoder()\n",
    "history = adapt_compile_and_fit(basic_encoder_decoder, train_ds, valid_ds, epochs=5)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate(model, sentence_en):\n",
    "    translation = \"\"\n",
    "    for word_idx in range(50):\n",
    "        X = np.array([sentence_en])  # encoder input\n",
    "        X_dec = np.array([\"startofseq \" + translation])  # decoder input\n",
    "        y_proba = model.predict((X, X_dec))[0, word_idx]  # last token's probas\n",
    "        predicted_word_id = np.argmax(y_proba)\n",
    "        predicted_word = model.vectorization_layer_es.get_vocabulary()[\n",
    "            predicted_word_id\n",
    "        ]\n",
    "        if predicted_word == \"endofseq\":\n",
    "            break\n",
    "        translation += \" \" + predicted_word\n",
    "    return translation.strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'les [UNK] de [UNK] ont été [UNK]'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translate(basic_encoder_decoder, \"Chocolate spreads have seen the largest\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
