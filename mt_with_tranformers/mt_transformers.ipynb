{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import data\n",
    "from tensorflow import keras\n",
    "from keras import layers\n",
    "\n",
    "K = keras.backend\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://storage.googleapis.com/download.tensorflow.org/data/spa-eng.zip\"\n",
    "path = keras.utils.get_file(\n",
    "    \"spa-eng.zip\", origin=url, cache_dir=\"datasets\", extract=True\n",
    ")\n",
    "text = (Path(path).with_name(\"spa-eng\") / \"spa.txt\").read_text(encoding=\"utf-8\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = text.replace(\"¡\", \"\").replace(\"¿\", \"\")\n",
    "pairs = [line.split(\"\\t\") for line in text.splitlines()]\n",
    "np.random.seed(42)\n",
    "np.random.shuffle(pairs)\n",
    "sentences_en, sentences_es = zip(*pairs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_encoder = tf.constant(sentences_en)\n",
    "X_decoder = tf.constant([f\"startofseq {s}\" for s in sentences_es])\n",
    "Y = tf.constant([f\"{s} endofseq\" for s in sentences_es])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adapt_compile_and_fit(\n",
    "    model, X_encoder, X_decoder, Y, batch_size=32, epochs=5, validation_size=0.2\n",
    "):\n",
    "    valid_len = int(validation_size * len(Y))\n",
    "    train_len = len(Y) - valid_len\n",
    "\n",
    "    X_train_encoder = X_encoder[:train_len]\n",
    "    X_train_decoder = X_decoder[:train_len]\n",
    "\n",
    "    X_valid_encoder = X_encoder[train_len:]\n",
    "    X_valid_decoder = X_decoder[train_len:]\n",
    "\n",
    "    Y_train = Y[:train_len]\n",
    "    Y_valid = Y[train_len:]\n",
    "\n",
    "    model.vectorization_layer_en.adapt(X_train_encoder)\n",
    "    model.vectorization_layer_es.adapt([f\"{s} endofseq\" for s in X_train_decoder])\n",
    "\n",
    "    Y_train = model.vectorization_layer_es(Y_train)\n",
    "    Y_valid = model.vectorization_layer_es(Y_valid)\n",
    "\n",
    "    model.compile(\n",
    "        loss=\"sparse_categorical_crossentropy\", optimizer=\"nadam\", metrics=[\"accuracy\"]\n",
    "    )\n",
    "\n",
    "    model.fit(\n",
    "        (X_train_encoder, X_train_decoder),\n",
    "        Y_train,\n",
    "        epochs=epochs,\n",
    "        batch_size=batch_size,\n",
    "        validation_data=((X_valid_encoder, X_valid_decoder), Y_valid),\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicEncoderDecoder(keras.Model):\n",
    "    def __init__(\n",
    "        self,\n",
    "        vocabulary_size=1000,\n",
    "        max_length=50,\n",
    "        embedding_size=128,\n",
    "        n_units_lstm=512,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "        self.vectorization_layer_en = layers.TextVectorization(\n",
    "            vocabulary_size, output_sequence_length=max_length\n",
    "        )\n",
    "        self.vectorization_layer_es = layers.TextVectorization(\n",
    "            vocabulary_size, output_sequence_length=max_length\n",
    "        )\n",
    "\n",
    "        self.encoder_embedding_layer = layers.Embedding(\n",
    "            vocabulary_size, embedding_size, mask_zero=True\n",
    "        )\n",
    "        self.decoder_embedding_layer = layers.Embedding(\n",
    "            vocabulary_size, embedding_size, mask_zero=True\n",
    "        )\n",
    "\n",
    "        self.encoder = layers.LSTM(n_units_lstm, return_state=True)\n",
    "        self.decoder = layers.LSTM(n_units_lstm, return_sequences=True)\n",
    "\n",
    "        self.output_layer = layers.Dense(vocabulary_size, activation=\"softmax\")\n",
    "\n",
    "    def call(self, inputs):\n",
    "        encoder_inputs, decoder_inputs = inputs\n",
    "\n",
    "        encoder_input_ids = self.vectorization_layer_en(encoder_inputs)\n",
    "        decoder_input_ids = self.vectorization_layer_es(decoder_inputs)\n",
    "\n",
    "        encoder_embeddings = self.encoder_embedding_layer(encoder_input_ids)\n",
    "        decoder_embeddings = self.decoder_embedding_layer(decoder_input_ids)\n",
    "\n",
    "        encoder_output, *encoder_state = self.encoder(encoder_embeddings)\n",
    "        decoder_output = self.decoder(decoder_embeddings, initial_state=encoder_state)\n",
    "\n",
    "        return self.output_layer(decoder_output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2975/2975 [==============================] - 78s 24ms/step - loss: 0.3946 - accuracy: 0.4530 - val_loss: 0.3055 - val_accuracy: 0.5312\n"
     ]
    }
   ],
   "source": [
    "K.clear_session()\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "basic_encoder_decoder = BasicEncoderDecoder()\n",
    "adapt_compile_and_fit(basic_encoder_decoder, X_encoder, X_decoder, Y, epochs=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BidirectionalEncoderDecoderWithAttention(keras.Model):\n",
    "    def __init__(\n",
    "        self,\n",
    "        vocabulary_size=1000,\n",
    "        max_length=50,\n",
    "        embedding_size=128,\n",
    "        n_units_lstm=512,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "        self.vectorization_layer_en = layers.TextVectorization(\n",
    "            vocabulary_size, output_sequence_length=max_length\n",
    "        )\n",
    "        self.vectorization_layer_es = layers.TextVectorization(\n",
    "            vocabulary_size, output_sequence_length=max_length\n",
    "        )\n",
    "\n",
    "        self.encoder_embedding_layer = layers.Embedding(\n",
    "            vocabulary_size, embedding_size, mask_zero=True\n",
    "        )\n",
    "        self.decoder_embedding_layer = layers.Embedding(\n",
    "            vocabulary_size, embedding_size, mask_zero=True\n",
    "        )\n",
    "\n",
    "        self.encoder = layers.Bidirectional(\n",
    "            layers.LSTM(n_units_lstm // 2, return_sequences=True, return_state=True)\n",
    "        )\n",
    "        self.decoder = layers.LSTM(n_units_lstm, return_sequences=True)\n",
    "        self.attention_layer = layers.Attention()\n",
    "        self.output_layer = layers.Dense(vocabulary_size, activation=\"softmax\")\n",
    "\n",
    "    def call(self, inputs):\n",
    "        encoder_inputs, decoder_inputs = inputs\n",
    "\n",
    "        encoder_input_ids = self.vectorization_layer_en(encoder_inputs)\n",
    "        decoder_input_ids = self.vectorization_layer_es(decoder_inputs)\n",
    "\n",
    "        encoder_embeddings = self.encoder_embedding_layer(encoder_input_ids)\n",
    "        decoder_embeddings = self.decoder_embedding_layer(decoder_input_ids)\n",
    "\n",
    "        encoder_output, *encoder_state = self.encoder(encoder_embeddings)\n",
    "        encoder_state = [\n",
    "            tf.concat(encoder_state[0::2], axis=-1),\n",
    "            tf.concat(encoder_state[1::2], axis=-1),\n",
    "        ]\n",
    "        decoder_output = self.decoder(decoder_embeddings, initial_state=encoder_state)\n",
    "        attention_output = self.attention_layer([decoder_output, encoder_output])\n",
    "\n",
    "        return self.output_layer(attention_output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2975/2975 [==============================] - 114s 35ms/step - loss: 0.3754 - accuracy: 0.4856 - val_loss: 0.2626 - val_accuracy: 0.5873\n"
     ]
    }
   ],
   "source": [
    "K.clear_session()\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "bidirect_encoder_decoder_with_attention = BidirectionalEncoderDecoderWithAttention()\n",
    "adapt_compile_and_fit(\n",
    "    bidirect_encoder_decoder_with_attention, X_encoder, X_decoder, Y, epochs=1\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(layers.Layer):\n",
    "    def __init__(self, max_length=50, embed_size=128, dtype=tf.float32, **kwargs):\n",
    "        super().__init__(dtype=dtype, **kwargs)\n",
    "        if not embed_size % 2 == 0:\n",
    "            raise ValueError(\"The `embedding_size` must be even.\")\n",
    "\n",
    "        p, i = np.meshgrid(np.arange(max_length), 2 * np.arange(embed_size // 2))\n",
    "        pos_emb = np.empty((1, max_length, embed_size))\n",
    "        pos_emb[:, :, 0::2] = np.sin(p / 10_000 ** (i / embed_size)).T\n",
    "        pos_emb[:, :, 1::2] = np.cos(p / 10_000 ** (i / embed_size)).T\n",
    "        self.positional_embedding = tf.constant(pos_emb.astype(self.dtype))\n",
    "        self.supports_masking = True\n",
    "\n",
    "    def call(self, inputs):\n",
    "        batch_max_length = tf.shape(inputs)[1]\n",
    "        return inputs + self.positional_embedding[:, :batch_max_length]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(layers.Layer):\n",
    "    def __init__(self, embed_size=128, n_heads=8, n_units=128, dropout=0.1, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.attn = layers.MultiHeadAttention(n_heads, embed_size, dropout=dropout)\n",
    "        self.add = layers.Add()\n",
    "        self.norm = layers.LayerNormalization()\n",
    "        self.dense1 = layers.Dense(n_units, \"relu\", kernel_initializer=\"he_normal\")\n",
    "        self.dense2 = layers.Dense(embed_size)\n",
    "        self.dropout = layers.Dropout(dropout)\n",
    "\n",
    "    def call(self, inputs, mask):\n",
    "        Z = inputs\n",
    "        skip_Z = Z\n",
    "        Z = self.attn(Z, value=Z, attention_mask=mask)\n",
    "        Z = self.norm(self.add([Z, skip_Z]))\n",
    "        skip_Z = Z\n",
    "        Z = self.dense1(Z)\n",
    "        Z = self.dense2(Z)\n",
    "        Z = self.dropout(Z)\n",
    "        return self.norm(self.add([Z, skip_Z]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(layers.Layer):\n",
    "    def __init__(\n",
    "        self, embed_size=128, n_heads=8, n_units=128, dropout_rate=0.1, **kwargs\n",
    "    ):\n",
    "        super().__init__(**kwargs)\n",
    "        self.attn1 = layers.MultiHeadAttention(n_heads, embed_size, dropout=dropout_rate)\n",
    "        self.attn2 = layers.MultiHeadAttention(n_heads, embed_size, dropout=dropout_rate)\n",
    "        self.add = layers.Add()\n",
    "        self.norm = layers.LayerNormalization()\n",
    "        self.dense1 = layers.Dense(n_units, \"relu\", kernel_initializer=\"he_normal\")\n",
    "        self.dense2 = layers.Dense(embed_size)\n",
    "        self.dropout = layers.Dropout(dropout_rate)\n",
    "\n",
    "    def call(self, inputs, mask):\n",
    "        decoder_mask, encoder_mask = mask\n",
    "        Z, encoder_output = inputs\n",
    "        Z_skip = Z\n",
    "        Z = self.attn1(Z, value=Z, attention_mask=decoder_mask)\n",
    "        Z = self.norm(self.add([Z, Z_skip]))\n",
    "        Z_skip = Z\n",
    "        Z = self.attn2(Z, value=encoder_output, attention_mask=encoder_mask)\n",
    "        Z = self.norm(self.add([Z, Z_skip]))\n",
    "        Z_skip = Z\n",
    "        Z = self.dense1(Z)\n",
    "        Z = self.dense2(Z)\n",
    "        #Z = self.dropout(Z)\n",
    "        return self.norm(self.add([Z, Z_skip]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import TextVectorization, Embedding\n",
    "\n",
    "\n",
    "class Transformer(keras.Model):\n",
    "    def __init__(\n",
    "        self,\n",
    "        vocab_size=1000,\n",
    "        max_length=50,\n",
    "        embed_size=128,\n",
    "        n_blocks=2,\n",
    "        n_heads=8,\n",
    "        n_units=128,\n",
    "        dropout_rate=0.1,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "        self.vectorization_layer_en = TextVectorization(\n",
    "            vocab_size, output_sequence_length=max_length\n",
    "        )\n",
    "        self.vectorization_layer_es = TextVectorization(\n",
    "            vocab_size, output_sequence_length=max_length\n",
    "        )\n",
    "        self.encoder_embedding = Embedding(vocab_size, embed_size, mask_zero=True)\n",
    "        self.decoder_embedding = Embedding(vocab_size, embed_size, mask_zero=True)\n",
    "        self.positional_embedding = PositionalEncoding(max_length, embed_size)\n",
    "        self.encoder_blocks = [\n",
    "            Encoder(embed_size, n_heads, n_units, dropout_rate) for _ in range(n_blocks)\n",
    "        ]\n",
    "        self.decoder_blocks = [\n",
    "            Decoder(embed_size, n_heads, n_units, dropout_rate) for _ in range(n_blocks)\n",
    "        ]\n",
    "        self.output_layer = layers.Dense(vocab_size, activation=\"softmax\")\n",
    "\n",
    "    def call(self, inputs):\n",
    "        encoder_inputs, decoder_inputs = inputs\n",
    "\n",
    "        encoder_input_ids = self.vectorization_layer_en(encoder_inputs)\n",
    "        decoder_input_ids = self.vectorization_layer_es(decoder_inputs)\n",
    "\n",
    "        encoder_embeddings = self.encoder_embedding(encoder_input_ids)\n",
    "        decoder_embeddings = self.decoder_embedding(decoder_input_ids)\n",
    "\n",
    "        encoder_pos_embeddings = self.positional_embedding(encoder_embeddings)\n",
    "        decoder_pos_embeddings = self.positional_embedding(decoder_embeddings)\n",
    "\n",
    "        encoder_mask = tf.math.not_equal(encoder_input_ids, 0)[:, tf.newaxis]\n",
    "        decoder_pad_mask = tf.math.not_equal(decoder_input_ids, 0)[:, tf.newaxis]\n",
    "        batch_max_len_decoder = tf.shape(decoder_embeddings)[1]\n",
    "        decoder_causal_mask = tf.linalg.band_part(  # Lower triangular matrix.\n",
    "            tf.ones((batch_max_len_decoder, batch_max_len_decoder), tf.bool), -1, 0\n",
    "        )\n",
    "        decoder_mask = decoder_causal_mask & decoder_pad_mask\n",
    "\n",
    "        Z = encoder_pos_embeddings\n",
    "        for encoder_block in self.encoder_blocks:\n",
    "            Z = encoder_block(Z, mask=encoder_mask)\n",
    "\n",
    "        encoder_output = Z\n",
    "        Z = decoder_pos_embeddings\n",
    "        for decoder_block in self.decoder_blocks:\n",
    "            Z = decoder_block([Z, encoder_output], mask=[decoder_mask, encoder_mask])\n",
    "\n",
    "        return self.output_layer(Z)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K.clear_session()\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "transformer = Transformer()\n",
    "adapt_compile_and_fit(transformer, X_encoder, X_decoder, Y, epochs=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
