{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "source = Path(\"data/en-fr.csv\")\n",
    "\n",
    "for i, chunk in enumerate(pd.read_csv(source, chunksize=100_000)):\n",
    "    chunk.to_csv(f\"data/en-fr-chunk-{i:03}.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepaths = [f\"data/en-fr-chunk-{i:03}.csv\" for i in range(226)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "en,fr\n",
      "Changing Lives | Changing Society | How It Works | Technology Drives Change Home | Concepts | Teachers | Search | Overview | Credits | HHCC Web | Reference | Feedback Virtual Museum of Canada Home Page,\"Il a transformé notre vie | Il a transformé la société | Son fonctionnement | La technologie, moteur du changement Accueil | Concepts | Enseignants | Recherche | Aperçu | Collaborateurs | Web HHCC | Ressources | Commentaires Musée virtuel du Canada\"\n",
      "Site map,Plan du site\n",
      "Feedback,Rétroaction\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\".join(open(filepaths[0], encoding=\"utf-8\").readlines()[:4]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_csv_line(line):\n",
    "    defaults = 2 * [tf.constant(\"\", dtype=tf.string)]\n",
    "    fields = tf.io.decode_csv(line, record_defaults=defaults)\n",
    "    return tf.stack(fields[0]), tf.stack(fields[1])\n",
    "\n",
    "\n",
    "def prepare_input_and_target(sentences_en, sentences_fr):\n",
    "    return (sentences_en, b\"startofseq \" + sentences_fr), sentences_fr + b\" endofseq\"\n",
    "\n",
    "\n",
    "def csv_files_dataset(\n",
    "    filepaths,\n",
    "    n_readers=8,\n",
    "    n_read_threads=None,\n",
    "    n_parse_threads=8,\n",
    "    batch_size=32,\n",
    "    shuffle_buffer_size=10_000,\n",
    "    seed=42,\n",
    "):\n",
    "    dataset = tf.data.Dataset.list_files(filepaths, seed=seed)\n",
    "    dataset = dataset.interleave(  # type: ignore\n",
    "        lambda filepath: tf.data.TextLineDataset(filepath).skip(1),\n",
    "        cycle_length=n_readers,\n",
    "        num_parallel_calls=n_read_threads,\n",
    "    )\n",
    "    dataset = dataset.map(parse_csv_line, num_parallel_calls=n_parse_threads)\n",
    "    dataset = dataset.map(prepare_input_and_target, num_parallel_calls=n_parse_threads)\n",
    "    dataset = dataset.shuffle(shuffle_buffer_size, seed=seed)\n",
    "    return dataset.batch(batch_size).prefetch(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_set = csv_files_dataset(filepaths[0:2])\n",
    "# example_set = example_set.take(2)\n",
    "# list(example_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary_size = 1000\n",
    "max_length = 50\n",
    "\n",
    "vectorization_layer_en = tf.keras.layers.TextVectorization(\n",
    "    max_tokens=vocabulary_size, output_sequence_length=max_length\n",
    ")\n",
    "vectorization_layer_es = tf.keras.layers.TextVectorization(\n",
    "    max_tokens=vocabulary_size, output_sequence_length=max_length\n",
    ")\n",
    "\n",
    "vectorization_layer_en.adapt(example_set.map(lambda sentences, target: sentences[0]))\n",
    "vectorization_layer_es.adapt(example_set.map(lambda sentences, target: sentences[1] + b\" endofseq\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['', '[UNK]', 'the', 'of', 'and', 'to', 'in', 'a', 'for', 'is']"
      ]
     },
     "execution_count": 336,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorization_layer_en.get_vocabulary()[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['', '[UNK]', 'de', 'startofseq', 'endofseq', 'la', 'et', 'les', 'des', 'le']"
      ]
     },
     "execution_count": 337,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorization_layer_es.get_vocabulary()[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(4, 50), dtype=int64, numpy=\n",
       "array([[18,  1,  1,  3,  1,  4,  1,  1, 10,  1,  1,  1,  1,  1,  1,  1,\n",
       "         8,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0],\n",
       "       [ 1,  1,  1, 11,  2,  1,  1, 10,  1, 11,  1,  1,  1,  1,  1,  1,\n",
       "         1,  8,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0],\n",
       "       [ 1,  1,  1, 19,  1,  1,  1, 11,  1, 10,  1, 28,  2,  1, 11,  1,\n",
       "         1, 65,  8,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0],\n",
       "       [ 1,  1,  1,  1,  1,  1,  1, 10,  1,  1,  5,  1, 11,  1,  5, 11,\n",
       "         1,  4, 18,  1, 19,  1,  3,  1, 10,  1,  1,  8,  0,  0,  0,  0,\n",
       "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0]], dtype=int64)>"
      ]
     },
     "execution_count": 311,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorization_layer_es(list(example_set.map(lambda sentences, target: target))[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_set = example_set.map(lambda sentences, target: (sentences, vectorization_layer_es(target)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[((<tf.Tensor: shape=(4,), dtype=string, numpy=\n",
       "   array([b'In 1974, the Council combined its astronomy and spectroscopy units and created the Herzberg Institute of Astrophysics, where he worked until his retirement in 1995.',\n",
       "          b'Organized the distribution of a survey of the Assembl\\xc3\\xa9e communautaire fransaskoise (ACF) to employees of this office willing to complete it.',\n",
       "          b'Brochures will be distributed to the Francophone community and more elaborate electronic versions will be available on the Internet.',\n",
       "          b'L AN'], dtype=object)>,\n",
       "   <tf.Tensor: shape=(4,), dtype=string, numpy=\n",
       "   array([b\"startofseq En 1974, le Conseil fusionne les unit\\xc3\\xa9s d'astronomie et de spectroscopie et cr\\xc3\\xa9e en son honneur l'Institut Herzberg d'Astrophysique ; il y travaille jusqu'\\xc3\\xa0 sa retraite, en 1995.\",\n",
       "          b\"startofseq Organisation de la distribution d'un sondage de l'Assembl\\xc3\\xa9e communautaire fransaskoise (ACF) aux employ\\xc3\\xa9s de ce bureau dispos\\xc3\\xa9s \\xc3\\xa0 y r\\xc3\\xa9pondre.\",\n",
       "          b\"startofseq Les d\\xc3\\xa9pliants seront distribu\\xc3\\xa9s aux communaut\\xc3\\xa9s francophones et des versions \\xc3\\xa9lectroniques plus \\xc3\\xa9labor\\xc3\\xa9es seront diffus\\xc3\\xa9es au moyen de l'Internet.\",\n",
       "          b'startofseq D\\xc2\\x92ELLESME'], dtype=object)>),\n",
       "  <tf.Tensor: shape=(4, 50), dtype=int64, numpy=\n",
       "  array([[ 6,  1, 58,  1,  1, 13,  1,  1,  7,  9,  1,  7,  1,  6,  1,  1,\n",
       "           1,  1,  1,  1,  1,  1,  1,  1,  1,  6,  1,  5,  0,  0,  0,  0,\n",
       "           0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "           0,  0],\n",
       "         [ 1,  9,  2,  1,  1,  1,  9,  1,  1,  1,  1,  1,  1,  9, 19,  1,\n",
       "           1,  3,  1,  1,  5,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "           0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "           0,  0],\n",
       "         [13,  1,  1,  1,  1,  1,  1,  7,  8,  1,  1, 48,  1,  1,  1, 91,\n",
       "           1,  9,  1,  5,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "           0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "           0,  0],\n",
       "         [ 1,  5,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "           0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "           0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "           0,  0]], dtype=int64)>),\n",
       " ((<tf.Tensor: shape=(4,), dtype=string, numpy=\n",
       "   array([b'The coordinator also met with the general director of Soci\\xc3\\xa9t\\xc3\\xa9 franco-manitobaine in July and attended the annual general meeting of the Soci\\xc3\\xa9t\\xc3\\xa9 franco-manitobaine (SFM) in October.',\n",
       "          b'\\xe2\\x80\\xa2 The Human Resources group helped organize the Rendez-vous de la Francophonie by taking care of some of the activities offered by the Department in order to raise awareness of and promote official languages.',\n",
       "          b'Each institution will train a group of 5 young people in mediation-based conflict resolution.',\n",
       "          b'Official language communities are kept informed about funding opportunities through the Department\\xe2\\x80\\x99s website, the Network of Regional and Program Coordinators and in the course of various meetings and communications.'],\n",
       "         dtype=object)>,\n",
       "   <tf.Tensor: shape=(4,), dtype=string, numpy=\n",
       "   array([b\"startofseq Enfin, le coordonnateur a rencontr\\xc3\\xa9 le directeur g\\xc3\\xa9n\\xc3\\xa9ral de la Soci\\xc3\\xa9t\\xc3\\xa9 franco-manitobaine au mois de juillet et il a assist\\xc3\\xa9 \\xc3\\xa0 l'assembl\\xc3\\xa9e g\\xc3\\xa9n\\xc3\\xa9rale annuelle de La Soci\\xc3\\xa9t\\xc3\\xa9 franco-manitobaine (SFM) en octobre.\",\n",
       "          b\"startofseq \\xe2\\x80\\xa2 Le groupe des ressources humaines a particip\\xc3\\xa9 \\xc3\\xa0 l'organisation des Rendez-vous de la Francophonie, en s'occupant de certaines des activit\\xc3\\xa9s offertes au Minist\\xc3\\xa8re dans le but de sensibiliser et de faire la promotion des langues officielles.\",\n",
       "          b'startofseq Chaque institution formera un groupe de cinq jeunes \\xc3\\xa0 la r\\xc3\\xa9solution des conflits par la m\\xc3\\xa9diation.',\n",
       "          b'startofseq Les communaut\\xc3\\xa9s de langue officielle sont tenues au courant des possibilit\\xc3\\xa9s de financement par le biais du site Web du Minist\\xc3\\xa8re, du r\\xc3\\xa9seau des coordonnateurs r\\xc3\\xa9gionaux et de programmes et par diverses rencontres et communications.'],\n",
       "         dtype=object)>),\n",
       "  <tf.Tensor: shape=(4, 50), dtype=int64, numpy=\n",
       "  array([[ 1, 58,  1, 20,  1, 58,  1,  1,  9,  2,  1,  1, 91,  1,  9,  1,\n",
       "           7,  1, 20,  1,  3,  1,  1,  1,  9,  2,  1,  1,  1,  6,  1,  5,\n",
       "           0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "           0,  0],\n",
       "         [ 1, 58,  1,  8,  1,  1, 20,  1,  3,  1,  8,  1,  9,  2,  1,  6,\n",
       "           1,  9,  1,  8,  1,  1, 91,  1,  1, 58,  1,  9,  1,  7,  9,  1,\n",
       "           2,  1,  8,  1,  1,  5,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "           0,  0],\n",
       "         [ 1,  1,  1, 29,  1,  9,  1,  1,  3,  2,  1,  8,  1, 11,  2,  1,\n",
       "           5,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "           0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "           0,  0],\n",
       "         [13,  1,  9,  1,  1,  1,  1, 91,  1,  8,  1,  9,  1, 11, 58,  1,\n",
       "           1,  1,  1,  1,  1,  1,  1,  8,  1,  1,  7,  9,  1,  7, 11,  1,\n",
       "           1,  7,  1,  5,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "           0,  0]], dtype=int64)>)]"
      ]
     },
     "execution_count": 327,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(example_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(42)  # extra code – ensures reproducibility on CPU\n",
    "encoder_inputs = tf.keras.layers.Input(shape=[], dtype=tf.string)\n",
    "decoder_inputs = tf.keras.layers.Input(shape=[], dtype=tf.string)\n",
    "\n",
    "embed_size = 128\n",
    "encoder_input_ids = vectorization_layer_en(encoder_inputs)\n",
    "decoder_input_ids = vectorization_layer_es(decoder_inputs)\n",
    "encoder_embedding_layer = tf.keras.layers.Embedding(\n",
    "    vocabulary_size, embed_size, mask_zero=True\n",
    ")\n",
    "decoder_embedding_layer = tf.keras.layers.Embedding(\n",
    "    vocabulary_size, embed_size, mask_zero=True\n",
    ")\n",
    "encoder_embeddings = encoder_embedding_layer(encoder_input_ids)\n",
    "decoder_embeddings = decoder_embedding_layer(decoder_input_ids)\n",
    "\n",
    "encoder = tf.keras.layers.LSTM(512, return_state=True)\n",
    "encoder_outputs, *encoder_state = encoder(encoder_embeddings)\n",
    "\n",
    "decoder = tf.keras.layers.LSTM(512, return_sequences=True)\n",
    "decoder_outputs = decoder(decoder_embeddings, initial_state=encoder_state)\n",
    "\n",
    "output_layer = tf.keras.layers.Dense(vocabulary_size, activation=\"softmax\")\n",
    "Y_proba = output_layer(decoder_outputs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6250/6250 [==============================] - 180s 28ms/step - loss: 1.3539 - accuracy: 0.3881\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x206317b0cd0>"
      ]
     },
     "execution_count": 340,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = tf.keras.Model(inputs=[encoder_inputs, decoder_inputs], outputs=[Y_proba])\n",
    "model.compile(\n",
    "    loss=\"sparse_categorical_crossentropy\", optimizer=\"nadam\", metrics=[\"accuracy\"]\n",
    ")\n",
    "model.fit(example_set, epochs=1)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
