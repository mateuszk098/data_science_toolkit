{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "code {\n",
       "    background: rgba(42, 53, 125, 0.10) !important;\n",
       "    border-radius: 4px !important;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# %load ../general_settings.py\n",
    "import os\n",
    "import shutil\n",
    "import subprocess\n",
    "import warnings\n",
    "from collections import defaultdict, namedtuple\n",
    "from copy import copy\n",
    "from functools import partial\n",
    "from itertools import combinations, product\n",
    "from pathlib import Path\n",
    "from time import strftime\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import plotly.figure_factory as ff\n",
    "import plotly.graph_objects as go\n",
    "import scipy.stats as stats\n",
    "from colorama import Fore, Style\n",
    "from IPython.core.display import HTML\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "ON_KAGGLE = os.getenv(\"KAGGLE_KERNEL_RUN_TYPE\") is not None\n",
    "\n",
    "# Colorama settings.\n",
    "CLR = (Style.BRIGHT + Fore.BLACK) if ON_KAGGLE else (Style.BRIGHT + Fore.WHITE)\n",
    "RED = Style.BRIGHT + Fore.RED\n",
    "BLUE = Style.BRIGHT + Fore.BLUE\n",
    "CYAN = Style.BRIGHT + Fore.CYAN\n",
    "RESET = Style.RESET_ALL\n",
    "\n",
    "FONT_COLOR = \"#010D36\"\n",
    "BACKGROUND_COLOR = \"#F6F5F5\"\n",
    "\n",
    "CELL_HOVER = {  # for row hover use <tr> instead of <td>\n",
    "    \"selector\": \"td:hover\",\n",
    "    \"props\": \"background-color: #F6F5F5\",\n",
    "}\n",
    "TEXT_HIGHLIGHT = {\n",
    "    \"selector\": \"td\",\n",
    "    \"props\": \"color: #FF2079; font-weight: bold\",\n",
    "}\n",
    "INDEX_NAMES = {\n",
    "    \"selector\": \".index_name\",\n",
    "    \"props\": \"font-style: italic; background-color: #010D36; color: #F2F2F0;\",\n",
    "}\n",
    "HEADERS = {\n",
    "    \"selector\": \"th:not(.index_name)\",\n",
    "    \"props\": \"font-style: italic; background-color: #010D36; color: #F2F2F0;\",\n",
    "}\n",
    "DF_STYLE = (INDEX_NAMES, HEADERS, TEXT_HIGHLIGHT)\n",
    "\n",
    "# Utility functions.\n",
    "def download_dataset_from_kaggle(user, dataset, directory):\n",
    "    command = \"kaggle datasets download -d \"\n",
    "    filepath = directory / (dataset + \".zip\")\n",
    "    if not filepath.is_file():\n",
    "        subprocess.run((command + user + \"/\" + dataset).split())\n",
    "        filepath.parent.mkdir(parents=True, exist_ok=True)\n",
    "        shutil.unpack_archive(dataset + \".zip\", \"data\")\n",
    "        shutil.move(dataset + \".zip\", \"data\")\n",
    "\n",
    "\n",
    "def download_competition_from_kaggle(competition):\n",
    "    command = \"kaggle competitions download -c \"\n",
    "    filepath = Path(\"data/\" + competition + \".zip\")\n",
    "    if not filepath.is_file():\n",
    "        subprocess.run((command + competition).split())\n",
    "        Path(\"data\").mkdir(parents=True, exist_ok=True)\n",
    "        shutil.unpack_archive(competition + \".zip\", \"data\")\n",
    "        shutil.move(competition + \".zip\", \"data\")\n",
    "\n",
    "\n",
    "# Html `code` block highlight. Must be included at the end of all imports!\n",
    "HTML(\n",
    "    \"\"\"\n",
    "<style>\n",
    "code {\n",
    "    background: rgba(42, 53, 125, 0.10) !important;\n",
    "    border-radius: 4px !important;\n",
    "}\n",
    "</style>\n",
    "\"\"\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import combinations, chain\n",
    "\n",
    "import shap\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.discriminant_analysis import StandardScaler\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.datasets import fetch_california_housing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <p style=\"padding: 20px; font-family: 'JetBrains Mono'; font-weight: bold; font-size: 100%; color: #08546c; letter-spacing: 2px; text-align: center; border-radius: 8px; border: 3px solid #08546c; background-color: #fffcfa\">Shapley Values</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<blockquote style=\"\n",
    "    margin-right: auto; \n",
    "    margin-left: auto; \n",
    "    background-color: #fffcfa;\n",
    "    padding: 20px;\n",
    "    border-radius: 8px;\n",
    "    border: 3px solid #08546c;\n",
    "\">\n",
    "<p style=\"\n",
    "    font-size: 20px;\n",
    "    font-family: 'JetBrains Mono';\n",
    "    color: #394d5f;\n",
    "    border-bottom: 2px solid #08546c;\n",
    "    margin-left: 15px;\n",
    "    margin-right: 15px;\n",
    "\">\n",
    "    <b>Can you explain why your model predicts such and not another value?</b> ðŸ¤”\n",
    "</p>\n",
    "\n",
    "<p style=\"\n",
    "    font-size: 16px;\n",
    "    font-family: 'JetBrains Mono';\n",
    "    color: #08546c;\n",
    "    margin-left: 15px;\n",
    "    margin-right: 15px;\n",
    "\">\n",
    "    Can you answer this question? If not, <b>SHAP</b> is something for you. I found out about <b>SHAP</b> maybe a week before this notebook was published. I noticed that in several notebooks but without any explanations. Therefore, I was just curious about what that library is and how to utilise it to understand models better.\n",
    "</p>\n",
    "    \n",
    "<p style=\"\n",
    "    font-size: 20px;\n",
    "    font-family: 'JetBrains Mono';\n",
    "    color: #394d5f;\n",
    "    border-bottom: 2px solid #08546c;\n",
    "    margin-left: 15px;\n",
    "    margin-right: 15px;\n",
    "\">\n",
    "    <b>Shapley values introduction</b> ðŸ’¡\n",
    "</p>\n",
    "\n",
    "<p style=\"\n",
    "    font-size: 16px;\n",
    "    font-family: 'JetBrains Mono';\n",
    "    color: #08546c;\n",
    "    margin-left: 15px;\n",
    "    margin-right: 15px;\n",
    "\">\n",
    "    Let's get started with this stuff. Generally, <b>SHAP</b> is a specific approach that helps you understand and explain your model's individual predictions. That method is based on <b>Shapley's values</b> from cooperative game theory. The point is that, we can consider prediction as a game where certain features are players and prediction is a gain. <b>SHAP</b> tells us how to distribute the contribution of each feature into certain prediction fairly. So, what exactly is a Shapley value? Well, it's an <b>average marginal contribution of a feature value across all possible coalitions of features.</b> Understandable? Of course, not. Let me explain this step by step.<br>\n",
    "    Let's suppose we need to build a machine learning model which has to predict house prices considering different attributes like median house age or average number of rooms. Assume we have only four features to make it as easy as possible. These are the <code>HouseAge</code> (median house age in block group), <code>AveRooms</code> (average number of rooms per household), <code>AveOccup</code> (average number of household members), and <code>Population</code> (block group population). These features are a subset of the California housing dataset from <code>scikit-learn</code>, which we will use later. Okay, so we've just built such a model and test it on a test dataset. It turns out that the average house price is $200,000$. After that, we got a new sample and must predict its price. We get $250,000$, and somebody ask why there is such a difference? <b>More precisely, how much each feature of this certain observation contributes to the prediction compared to the average price?</b><br>\n",
    "    In the case of linear models, the answer is easy, i.e. the prediction is just the sum of the weight of each feature times the feature value plus intercept. Weights and intercept are found by minimizing the cost function. Nevertheless, in the case of boosted trees or neural networks, the answer is a challenge. For such cases, the problem of model explainability is solved by a method from cooperative game theory - this is the place where <b>SHAP</b> comes into play. It explains the difference between specific prediction and average prediction (made for the whole dataset).\n",
    "</p>\n",
    "    \n",
    "<p style=\"\n",
    "    font-size: 20px;\n",
    "    font-family: 'JetBrains Mono';\n",
    "    color: #394d5f;\n",
    "    border-bottom: 2px solid #08546c;\n",
    "    margin-left: 15px;\n",
    "    margin-right: 15px;\n",
    "\">\n",
    "    <b>How to calculate Shapley value?</b> ðŸ•µ\n",
    "</p>\n",
    "\n",
    "<p style=\"\n",
    "    font-size: 16px;\n",
    "    font-family: 'JetBrains Mono';\n",
    "    color: #08546c;\n",
    "    margin-left: 15px;\n",
    "    margin-right: 15px;\n",
    "\">\n",
    "    Calculation of Shapley value for a specific feature is based on coalitions of features. Let's get back to the example with house prices, and suppose we want to calculate the Shapley value for a <code>HouseAge</code> feature for a specific sample. Firstly, consider the coalition of <code>AveRooms</code> and <code>Population</code>. As we want to assess the contribution of <code>HouseAge</code> (for example $30$ for this sample), we take values of these three features for that sample and randomly pick some value for the <code>AveOccup</code> from another sample. We evaluate the model and get a prediction of $220,000$. After that, we remove <code>HouseAge</code> from the coalition by picking the value for that feature from the sample drawn earlier (let's say <code>HouseAge</code> is $40$ there). We re-evaluate the model and get $200,000$. The contribution of <code>HouseAge</code> component is +$20,000$ in this case. These calculations depend on randomly drawn samples, and we should perform many test iterations to get a more reliable result. Moreover, we repeat this calculation for all possible coalitions of features. These coalitions are:\n",
    "<ul style=\"\n",
    "    font-size: 16px;\n",
    "    font-family: 'JetBrains Mono';\n",
    "    color: #08546c;\n",
    "    margin-left: 15px;\n",
    "    margin-right: 15px;\n",
    "\">\n",
    "    <li>No coalition of features.</li>\n",
    "    <li><code>AveRooms</code>.</li>\n",
    "    <li><code>AveOccup</code>.</li>\n",
    "    <li><code>Population</code>.</li>\n",
    "    <li><code>AveRooms</code>, <code>AveOccup</code>.</li>\n",
    "    <li><code>AveRooms</code>, <code>Population</code>.</li>\n",
    "    <li><code>AveOccup</code>, <code>Population</code>.</li>\n",
    "    <li><code>AveRooms</code>, <code>AveOccup</code>, <code>Population</code>.</li>\n",
    "</ul>\n",
    "</p>\n",
    "\n",
    "<p style=\"\n",
    "    font-size: 16px;\n",
    "    font-family: 'JetBrains Mono';\n",
    "    color: #08546c;\n",
    "    margin-left: 15px;\n",
    "    margin-right: 15px;\n",
    "\">\n",
    "For all of these coalitions, we compute the predicted price, with and without feature <code>HouseAge</code> value and take marginal contribution as the difference between prices. Let me illustrate this with example of California housing dataset.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = fetch_california_housing(return_X_y=True, as_frame=True)\n",
    "\n",
    "# Let's take only four features.\n",
    "features = [\"HouseAge\", \"AveRooms\", \"Population\", \"AveOccup\"]\n",
    "\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(\n",
    "    X[features], y, test_size=0.1, random_state=42\n",
    ")\n",
    "\n",
    "scaler = StandardScaler().set_output(transform=\"pandas\")\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_valid = scaler.transform(X_valid)\n",
    "\n",
    "model = LinearRegression().fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<blockquote style=\"\n",
    "    margin-right: auto; \n",
    "    margin-left: auto; \n",
    "    background-color: #fffcfa;\n",
    "    padding: 20px;\n",
    "    border-radius: 8px;\n",
    "    border: 3px solid #08546c;\n",
    "\">\n",
    "<p style=\"\n",
    "    font-size: 20px;\n",
    "    font-family: 'JetBrains Mono';\n",
    "    color: #394d5f;\n",
    "    border-bottom: 2px solid #08546c;\n",
    "    margin-left: 15px;\n",
    "    margin-right: 15px;\n",
    "\">\n",
    "    <b>What we exactly want to do?</b> ðŸ¤”\n",
    "</p>\n",
    "\n",
    "<p style=\"\n",
    "    font-size: 16px;\n",
    "    font-family: 'JetBrains Mono';\n",
    "    color: #08546c;\n",
    "    margin-left: 15px;\n",
    "    margin-right: 15px;\n",
    "\">\n",
    "    We want to calculate the Shapley value for a specific sample from the validation dataset by hand and then compare it to the Shapley value obtained from <code>shap</code> library. In this case, we want to explain contribution of <code>HouseAge</code> (which is the first feature - $0$).\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[37mAll possible coalitions of features (without `HouseAge`)\n",
      "\u001b[1m\u001b[31m=> () (1,) (2,) (3,) (1, 2) (1, 3) (2, 3) (1, 2, 3)\n"
     ]
    }
   ],
   "source": [
    "sample_no = 0  # Number of sample from valid dataset.\n",
    "feature_no = 0  # Number of feature, which contribution we want to explain.\n",
    "n_features = len(X_train.columns)\n",
    "\n",
    "feature_ids = list(range(n_features))\n",
    "feature_ids.remove(feature_no)\n",
    "\n",
    "nested_combs = list(list(combinations(feature_ids, r)) for r in range(0, n_features))\n",
    "features_combs = np.array(list(chain(*nested_combs)), dtype=object)\n",
    "all_features = np.arange(n_features)\n",
    "\n",
    "print(CLR + \"All possible coalitions of features (without `HouseAge`)\")\n",
    "print(RED + \"=>\", *features_combs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<blockquote style=\"\n",
    "    margin-right: auto; \n",
    "    margin-left: auto; \n",
    "    background-color: #fffcfa;\n",
    "    padding: 20px;\n",
    "    border-radius: 8px;\n",
    "    border: 3px solid #08546c;\n",
    "\">\n",
    "<p style=\"\n",
    "    font-size: 20px;\n",
    "    font-family: 'JetBrains Mono';\n",
    "    color: #394d5f;\n",
    "    border-bottom: 2px solid #08546c;\n",
    "    margin-left: 15px;\n",
    "    margin-right: 15px;\n",
    "\">\n",
    "    <b>Monte Carlo estimation of Shapley value</b> ðŸ“œ\n",
    "</p>\n",
    "\n",
    "<p style=\"\n",
    "    font-size: 16px;\n",
    "    font-family: 'JetBrains Mono';\n",
    "    color: #08546c;\n",
    "    margin-left: 15px;\n",
    "    margin-right: 15px;\n",
    "\">\n",
    "    In the case of a large number of features, the calculation for all coalitions is complex. However, there is an approximate method for that, based on <b>Monte-Carlo sampling</b>. This method calculates Shapley value for certain feature $j$ and certain sample $x$:\n",
    "    \\[\\hat{\\phi}_j(x) = \\frac{1}{N}\\sum_{n=1}^N \\left[\\hat{f}(x_{+j}^n) - \\hat{f}(x_{-j}^n)\\right]\\]\n",
    "where $N$ is the number of sampling steps, $\\hat{f}(x_{+j}^n)$ is the prediction for sample $x$ in the step $n$, where the part of feature values from this sample has been replaced by values from randomly picked observation, but value of feature $j$ still comes from $x$. The second part: $\\hat{f}(x_{-j}^n)$, is similar to the first one but value of feature $j$ is replaced by value from that randomly chosen observation.<br>\n",
    "In our case, we have only four features, so all combinations are calculable in a short time, so we perform something between Monte-Carlo sampling and direct calculation.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[37mShapley value by hand: \u001b[1m\u001b[31m-0.05325\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "n_iterations = 2000\n",
    "marginal_contributions = np.zeros(n_iterations)\n",
    "\n",
    "for i in range(n_iterations):\n",
    "    random_no = np.random.randint(len(X_valid))\n",
    "\n",
    "    x_features = np.array(np.random.choice(features_combs))  # Coalitions\n",
    "    z_features = np.setdiff1d(all_features, x_features)  # Feature values from random observation.\n",
    "\n",
    "    x = X_valid.iloc[sample_no, x_features]  # Sample we examine.\n",
    "    z = X_valid.iloc[random_no, z_features]  # Random sample, which serves as a donor.\n",
    "\n",
    "    sample = pd.DataFrame({**x.to_dict(), **z.to_dict()}, index=[0])[X_valid.columns]\n",
    "\n",
    "    sample_p_j = sample.copy()  # Sample with original value of feature `j`.\n",
    "    sample_m_j = sample.copy()  # Sample with replaced value of feature `j`.\n",
    "\n",
    "    sample_p_j.iloc[:, feature_no] = X_valid.iloc[sample_no, feature_no]\n",
    "    sample_m_j.iloc[:, feature_no] = X_valid.iloc[random_no, feature_no]\n",
    "\n",
    "    marginal_contributions[i] = model.predict(sample_p_j) - model.predict(sample_m_j)\n",
    "\n",
    "print(CLR + \"Shapley value by hand:\", RED + f\"{marginal_contributions.mean():.5f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[37m SHAP Output:\n",
      "\n",
      " .values =\n",
      "array([[-0.06180386, -0.08104377, -0.00380141, -0.00219214]])\n",
      "\n",
      ".base_values =\n",
      "array([2.06937846])\n",
      "\n",
      ".data =\n",
      "array([[-0.28671279, -0.49353058, -0.03075389,  0.0726769 ]])\n",
      "\n",
      "\u001b[1m\u001b[37mShapley value from SHAP library: \u001b[1m\u001b[31m-0.06180\n"
     ]
    }
   ],
   "source": [
    "explainer = shap.Explainer(model.predict, X_valid, seed=42)\n",
    "shap_values = explainer(X_valid.iloc[[sample_no], :])\n",
    "print(CLR, \"SHAP Output:\\n\\n\", shap_values)\n",
    "print()\n",
    "print(CLR + \"Shapley value from SHAP library:\", RED + f\"{shap_values.values[0, feature_no]:.5f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<blockquote style=\"\n",
    "    margin-right: auto; \n",
    "    margin-left: auto; \n",
    "    background-color: #fffcfa;\n",
    "    padding: 20px;\n",
    "    border-radius: 8px;\n",
    "    border: 3px solid #08546c;\n",
    "\">\n",
    "<p style=\"\n",
    "    font-size: 20px;\n",
    "    font-family: 'JetBrains Mono';\n",
    "    color: #394d5f;\n",
    "    border-bottom: 2px solid #08546c;\n",
    "    margin-left: 15px;\n",
    "    margin-right: 15px;\n",
    "\">\n",
    "    <b>Observations</b> ðŸ“”\n",
    "</p>\n",
    "\n",
    "<p style=\"\n",
    "    font-size: 16px;\n",
    "    font-family: 'JetBrains Mono';\n",
    "    color: #08546c;\n",
    "    margin-left: 15px;\n",
    "    margin-right: 15px;\n",
    "\">\n",
    "    Using <code>shap</code> library, the Shapley value is hidden in the <code>values</code> attribute of the explainer. We analyse the first feature, so it's the first value of that array. As you can see, perhaps the result calculated by hand is not a world champion, but these results are comparable. I checked several different configurations, and these values follow each other.<br>\n",
    "    Shapley value interpretation may be confusing. <b>The interpretation is that the value of feature $j$ contributed in different coalitions by $\\hat{\\phi}_j(x)$ to the prediction for this specific observation, in comparison to the average prediction.</b> It's not the difference between predictions when we remove this feature from the dataset.\n",
    "</p>\n",
    "<p style=\"\n",
    "    font-size: 20px;\n",
    "    font-family: 'JetBrains Mono';\n",
    "    color: #394d5f;\n",
    "    border-bottom: 2px solid #08546c;\n",
    "    margin-left: 15px;\n",
    "    margin-right: 15px;\n",
    "\">\n",
    "    <b>Shapley value properties</b> ðŸ’¡\n",
    "</p>\n",
    "\n",
    "<p style=\"\n",
    "    font-size: 16px;\n",
    "    font-family: 'JetBrains Mono';\n",
    "    color: #08546c;\n",
    "    margin-left: 15px;\n",
    "    margin-right: 15px;\n",
    "\">\n",
    "    Shapley value fulfills four axioms. These are:\n",
    "<ul style=\"\n",
    "    font-size: 16px;\n",
    "    font-family: 'JetBrains Mono';\n",
    "    color: #08546c;\n",
    "    margin-left: 15px;\n",
    "    margin-right: 15px;\n",
    "\">\n",
    "    <li><b>Efficiency</b> - feature contributions must sum up to the difference of sample prediction and average prediction.</li>\n",
    "    <li><b>Symmetry</b> - if two fature values equally contribute in all coalitions, then contribution of these should be equal.</li>\n",
    "    <li><b>Dummy</b> - the Shapley value of the feature value is equal to zero if it doesn't change anything during prediction in different coalitions.</li>\n",
    "    <li><b>Additivity</b> - in the example of a random forest - if we calculate the Shapley value for each tree individually, average it and then calculate the Shapley value for a random forest, they are the same.</li>\n",
    "</ul>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[37mAverage model prediction for valid dataset: \u001b[1m\u001b[31m2.06921\n"
     ]
    }
   ],
   "source": [
    "avg_prediction = model.predict(X_valid).mean()\n",
    "print(CLR + \"Average model prediction for valid dataset:\", RED + f\"{avg_prediction:.5f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[37mModel prediction for chosen sample: \u001b[1m\u001b[31m1.92054\n"
     ]
    }
   ],
   "source": [
    "sample_prediction = model.predict(X_valid.iloc[[sample_no], :])[0]\n",
    "print(CLR + \"Model prediction for chosen sample:\", RED + f\"{sample_prediction:.5f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[37mSum of average prediction and all Shapley values: \u001b[1m\u001b[31m1.92037\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    CLR + \"Sum of average prediction and all Shapley values:\",\n",
    "    RED + f\"{avg_prediction + shap_values.values.sum():.5f}\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<blockquote style=\"\n",
    "    margin-right: auto; \n",
    "    margin-left: auto; \n",
    "    background-color: #fffcfa;\n",
    "    padding: 20px;\n",
    "    border-radius: 8px;\n",
    "    border: 3px solid #08546c;\n",
    "\">\n",
    "<p style=\"\n",
    "    font-size: 20px;\n",
    "    font-family: 'JetBrains Mono';\n",
    "    color: #394d5f;\n",
    "    border-bottom: 2px solid #08546c;\n",
    "    margin-left: 15px;\n",
    "    margin-right: 15px;\n",
    "\">\n",
    "    <b>What is Shapley value?</b> ðŸ¤”\n",
    "</p>\n",
    "\n",
    "<p style=\"\n",
    "    font-size: 16px;\n",
    "    font-family: 'JetBrains Mono';\n",
    "    color: #08546c;\n",
    "    margin-left: 15px;\n",
    "    margin-right: 15px;\n",
    "\">\n",
    "    <b>The Shapley value is the average marginal contribution of a feature value across all possible coalitions of features.</b><br>\n",
    "    Understandable? I hope so.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <p style=\"padding: 20px; font-family: 'JetBrains Mono'; font-weight: bold; font-size: 100%; color: #08546c; letter-spacing: 2px; text-align: center; border-radius: 8px; border: 3px solid #08546c; background-color: #fffcfa\">SHAP in Action</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
